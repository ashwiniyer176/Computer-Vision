{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before one-hot encoding:  (50000, 1)\n",
      "Shape after one-hot encoding:  (50000, 10)\n",
      "391/391 [==============================] - 63s 160ms/step - loss: 1.7039 - accuracy: 0.3643 - val_loss: 1.3163 - val_accuracy: 0.5268\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2920faad6a0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# loading the dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# # building the input vector from the 32x32 pixels\n",
    "X_train = X_train.reshape(X_train.shape[0], 32, 32, 3)\n",
    "X_test = X_test.reshape(X_test.shape[0], 32, 32, 3)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# normalizing the data to help with the training\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# one-hot encoding using keras' numpy-related utilities\n",
    "n_classes = 10\n",
    "print(\"Shape before one-hot encoding: \", y_train.shape)\n",
    "Y_train = np_utils.to_categorical(y_train, n_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, n_classes)\n",
    "print(\"Shape after one-hot encoding: \", Y_train.shape)\n",
    "\n",
    "# building a linear stack of layers with the sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# convolutional layer\n",
    "model.add(Conv2D(50, kernel_size=(3, 3), strides=(1, 1),\n",
    "          padding='same', activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# convolutional layer\n",
    "model.add(Conv2D(75, kernel_size=(3, 3), strides=(\n",
    "    1, 1), padding='same', activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(125, kernel_size=(3, 3), strides=(\n",
    "    1, 1), padding='same', activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# flatten output of conv\n",
    "model.add(Flatten())\n",
    "\n",
    "# hidden layer\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "# output layer\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# compiling the sequential model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "# training the model for 10 epochs\n",
    "model.fit(X_train, Y_train, batch_size=128,\n",
    "          epochs=1, validation_data=(X_test, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "\n",
    "    # loading the dataset\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "    # # building the input vector from the 32x32 pixels\n",
    "    X_train = X_train.reshape(X_train.shape[0], 32, 32, 3)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 32, 32, 3)\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "\n",
    "    # normalizing the data to help with the training\n",
    "    X_train /= 255\n",
    "    X_test /= 255\n",
    "\n",
    "    n_classes = 10\n",
    "    print(\"Shape before one-hot encoding: \", y_train.shape)\n",
    "    Y_train = np_utils.to_categorical(y_train, n_classes)\n",
    "    Y_test = np_utils.to_categorical(y_test, n_classes)\n",
    "    print(\"Shape after one-hot encoding: \", Y_train.shape)\n",
    "\n",
    "    return X_train,y_train,X_test,y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    model = Sequential()\n",
    "    # convolutional layer\n",
    "    model.add(Conv2D(50, kernel_size=(3, 3), strides=(1, 1),\n",
    "            padding='same', activation='relu', input_shape=(32, 32, 3)))\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(75, kernel_size=(3, 3), strides=(\n",
    "        1, 1), padding='same', activation='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(125, kernel_size=(3, 3), strides=(\n",
    "        1, 1), padding='same', activation='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(250, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'],\n",
    "        optimizer='adam'\n",
    "        )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, Y_train,\n",
    "        batch_size={{choice([64, 128])}},\n",
    "        verbose=2,\n",
    "        validation_data=(X_test, Y_test)\n",
    "        )\n",
    "        \n",
    "    score, acc = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "from __future__ import print_function\n",
      "\n",
      "try:\n",
      "    from keras.datasets import cifar10\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import np_utils\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import np_utils\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'batch_size': hp.choice('batch_size', [64, 128]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: \n",
      "  3: # loading the dataset\n",
      "  4: (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
      "  5: \n",
      "  6: # # building the input vector from the 32x32 pixels\n",
      "  7: X_train = X_train.reshape(X_train.shape[0], 32, 32, 3)\n",
      "  8: X_test = X_test.reshape(X_test.shape[0], 32, 32, 3)\n",
      "  9: X_train = X_train.astype('float32')\n",
      " 10: X_test = X_test.astype('float32')\n",
      " 11: \n",
      " 12: # normalizing the data to help with the training\n",
      " 13: X_train /= 255\n",
      " 14: X_test /= 255\n",
      " 15: \n",
      " 16: n_classes = 10\n",
      " 17: print(\"Shape before one-hot encoding: \", y_train.shape)\n",
      " 18: Y_train = np_utils.to_categorical(y_train, n_classes)\n",
      " 19: Y_test = np_utils.to_categorical(y_test, n_classes)\n",
      " 20: print(\"Shape after one-hot encoding: \", Y_train.shape)\n",
      " 21: \n",
      " 22: \n",
      " 23: \n",
      " 24: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     model = Sequential()\n",
      "   4:     # convolutional layer\n",
      "   5:     model.add(Conv2D(50, kernel_size=(3, 3), strides=(1, 1),\n",
      "   6:             padding='same', activation='relu', input_shape=(32, 32, 3)))\n",
      "   7:     model.add(MaxPool2D(pool_size=(2, 2)))\n",
      "   8:     model.add(Dropout(0.25))\n",
      "   9: \n",
      "  10:     model.add(Conv2D(75, kernel_size=(3, 3), strides=(\n",
      "  11:         1, 1), padding='same', activation='relu'))\n",
      "  12:     model.add(MaxPool2D(pool_size=(2, 2)))\n",
      "  13:     model.add(Dropout(0.25))\n",
      "  14:     model.add(Conv2D(125, kernel_size=(3, 3), strides=(\n",
      "  15:         1, 1), padding='same', activation='relu'))\n",
      "  16: \n",
      "  17:     model.add(MaxPool2D(pool_size=(2, 2)))\n",
      "  18:     model.add(Dropout(0.25))\n",
      "  19:     model.add(Flatten())\n",
      "  20: \n",
      "  21:     model.add(Dense(500, activation='relu'))\n",
      "  22:     model.add(Dropout(0.4))\n",
      "  23:     model.add(Dense(250, activation='relu'))\n",
      "  24:     model.add(Dropout(0.3))\n",
      "  25:     model.add(Dense(10, activation='softmax'))\n",
      "  26:     model.compile(\n",
      "  27:         loss='categorical_crossentropy',\n",
      "  28:         metrics=['accuracy'],\n",
      "  29:         optimizer='adam'\n",
      "  30:         )\n",
      "  31: \n",
      "  32:     model.fit(\n",
      "  33:         X_train, Y_train,\n",
      "  34:         batch_size=space['batch_size'],\n",
      "  35:         verbose=2,\n",
      "  36:         validation_data=(X_test, Y_test)\n",
      "  37:         )\n",
      "  38:         \n",
      "  39:     score, acc = model.evaluate(X_test, Y_test, verbose=0)\n",
      "  40:     print('Test accuracy:', acc)\n",
      "  41:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  42: \n",
      "Shape before one-hot encoding:  (50000, 1)\n",
      "Shape after one-hot encoding:  (50000, 10)\n",
      "782/782 - 67s - loss: 1.6414 - accuracy: 0.3888 - val_loss: 1.2276 - val_accuracy: 0.5618\n",
      "\n",
      "Test accuracy:\n",
      "0.5618000030517578\n",
      "782/782 - 64s - loss: 1.6043 - accuracy: 0.4052 - val_loss: 1.2058 - val_accuracy: 0.5743\n",
      "\n",
      "Test accuracy:\n",
      "0.5742999911308289\n",
      "391/391 - 65s - loss: 1.7539 - accuracy: 0.3507 - val_loss: 1.3706 - val_accuracy: 0.5048\n",
      "\n",
      "Test accuracy:\n",
      "0.504800021648407\n",
      "391/391 - 61s - loss: 1.7524 - accuracy: 0.3465 - val_loss: 1.3805 - val_accuracy: 0.4965\n",
      "\n",
      "Test accuracy:\n",
      "0.4964999854564667\n",
      "391/391 - 62s - loss: 1.7243 - accuracy: 0.3585 - val_loss: 1.3020 - val_accuracy: 0.5161\n",
      "\n",
      "Test accuracy:\n",
      "0.5160999894142151\n",
      "782/782 - 64s - loss: 1.6563 - accuracy: 0.3848 - val_loss: 1.2290 - val_accuracy: 0.5576\n",
      "\n",
      "Test accuracy:\n",
      "0.5576000213623047\n",
      "391/391 - 61s - loss: 1.7105 - accuracy: 0.3651 - val_loss: 1.3526 - val_accuracy: 0.5081\n",
      "\n",
      "Test accuracy:\n",
      "0.5080999732017517\n",
      "782/782 - 73s - loss: 1.6265 - accuracy: 0.3987 - val_loss: 1.2393 - val_accuracy: 0.5571\n",
      "\n",
      "Test accuracy:\n",
      "0.5570999979972839\n",
      "782/782 - 66s - loss: 1.6617 - accuracy: 0.3813 - val_loss: 1.2421 - val_accuracy: 0.5531\n",
      "\n",
      "Test accuracy:\n",
      "0.5530999898910522\n",
      "391/391 - 63s - loss: 1.6874 - accuracy: 0.3753 - val_loss: 1.3293 - val_accuracy: 0.5245\n",
      "\n",
      "Test accuracy:\n",
      "0.5245000123977661\n",
      "100%|██████████| 10/10 [11:21<00:00, 68.15s/trial, best loss: -0.5742999911308289]\n"
     ]
    }
   ],
   "source": [
    "best_run, best_model = optim.minimize(\n",
    "    model=model,\n",
    "    data=data,\n",
    "    max_evals=10,\n",
    "    algo=tpe.suggest,\n",
    "    notebook_name='Automated Tuning',  \n",
    "    trials=Trials()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f375280c252d8492f9929f545102be4436b56b7b9d809e2d50f7485678504f56"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
